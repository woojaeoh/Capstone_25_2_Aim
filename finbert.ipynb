{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e078f01c-8b0c-476a-8749-4c2cfa095b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì„ íƒ] ë§¤ë„: 2343ê°œ (ìµœì‹ ìˆœ)\n",
      "[ì„ íƒ] ë§¤ìˆ˜: 2350ê°œ (ìµœì‹ ìˆœ)\n",
      "[ì„¸ì´í”„ê°€ë“œ] ìµœì¢… ìœ ì§€ 4671ê°œ / ìŠ¤í‚µ 22ê°œ (ê¸°ì¤€: 80ì)\n",
      "[ì €ì¥ ì™„ë£Œ] C:\\Users\\bsbcn\\Desktop\\ê²°ê³¼_ë§¤ìˆ˜ë§¤ë„_í•™ìŠµ\\preprocessed.json\n",
      "[ë””ë²„ê·¸] C:\\Users\\bsbcn\\Desktop\\ê²°ê³¼_ë§¤ìˆ˜ë§¤ë„_í•™ìŠµ\\preprocess_debug.tsv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ì „ì²˜ë¦¬ ì „ìš© (í†µí•© ì‹¤í–‰)\n",
    "ë¡œì§(ìœ ì§€):\n",
    "  - ëª¨ë“  ì¤„ì„ 'í•œ ì¤„'ë¡œ ë³‘í•© â†’ ë§ì¶¤ë²•/ë„ì–´ì“°ê¸° êµì •\n",
    "  - 'ëª©í‘œì£¼ê°€/ëª©í‘œê°€/TP(ë‹¨ì–´ê²½ê³„)' â†’ 'num' ì¹˜í™˜\n",
    "  - ìˆ«ì â†’ 'num' ì¹˜í™˜\n",
    "\n",
    "ë¶€ìˆ˜ ë¡œì§(ê¸°ì¡´ ìŠ¤íƒ€ì¼ ìœ ì§€):\n",
    "  - ë§¤ìˆ˜/ë§¤ë„ ê°ê° ìµœì‹  LIMIT_PER_CLASSê°œ ì„ íƒ\n",
    "  - ì„¸ì´í”„ê°€ë“œ: A) FULL â†’ B) êµì •ë§Œ â†’ C) ë³‘í•©ë§Œ â†’ D) ê¸°ë³¸ì •ë¦¬ë§Œ â†’ E) ìŠ¤í‚µ\n",
    "  - ì¶œë ¥: preprocessed.json, preprocess_debug.tsv\n",
    "\n",
    "ì…ë ¥ í´ë”:\n",
    "  ~/Desktop/ë§¤ë„ , ~/Desktop/ë§¤ìˆ˜   (ê° í´ë”ì˜ .txt)\n",
    "ì¶œë ¥:\n",
    "  ~/Desktop/ê²°ê³¼_ë§¤ìˆ˜ë§¤ë„_í•™ìŠµ/preprocessed.json\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== ê²½ë¡œ/ì„¤ì • =====\n",
    "HOME = Path.home()\n",
    "NEG_DIR = HOME / \"Desktop\" / \"ë§¤ë„\"\n",
    "POS_DIR = HOME / \"Desktop\" / \"ë§¤ìˆ˜\"\n",
    "OUT_DIR = HOME / \"Desktop\" / \"ê²°ê³¼_ë§¤ìˆ˜ë§¤ë„_í•™ìŠµ\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LIMIT_PER_CLASS = 2350    \n",
    "MIN_CHARS_AFTER  = 80\n",
    "\n",
    "# ===== ì¸ì½”ë”© ìŠ¤ë§ˆíŠ¸ ë¦¬ë” =====\n",
    "def read_text_smart(p: Path) -> str:\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"cp949\", \"euc-kr\", \"iso-8859-1\"]\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return p.read_text(encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    try:\n",
    "        return p.read_bytes().decode(\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# ===== ê¸°ë³¸ ì •ë¦¬(ê°œí–‰ì€ ë‚˜ì¤‘ì— ì œê±°) =====\n",
    "def basic_trim_keep_newlines(t: str) -> str:\n",
    "    t = re.sub(r'[\\u00A0\\u200B-\\u200D]+', ' ', t)       \n",
    "    t = t.replace('\\r\\n', '\\n').replace('\\r', '\\n')     \n",
    "    t = re.sub(r'[ \\t]+', ' ', t)                   \n",
    "    return t.strip()\n",
    "\n",
    "# ===== ëª¨ë“  ì¤„ì„ 'í•œ ì¤„'ë¡œ ë³‘í•© =====\n",
    "def merge_all_to_one_line(text: str) -> str:\n",
    "    t = basic_trim_keep_newlines(text)\n",
    "    t = t.replace('\\n', '')                           \n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t\n",
    "\n",
    "# ===== ë§ì¶¤ë²•/ë„ì–´ì“°ê¸° êµì • =====\n",
    "def correct_text_single_line(text: str) -> str:\n",
    "    txt = text\n",
    "    try:\n",
    "        from pykospacing import Spacing\n",
    "        txt = Spacing()(txt)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from hanspell import spell_checker\n",
    "        res = spell_checker.check(txt)\n",
    "        if hasattr(res, \"checked\") and isinstance(res.checked, str):\n",
    "            txt = res.checked\n",
    "    except Exception:\n",
    "        pass\n",
    "    return txt.strip()\n",
    "\n",
    "# ===== 'ëª©í‘œê°€/TP' â†’ 'num' ì¹˜í™˜ (tpëŠ” ë‹¨ì–´ ê²½ê³„) =====\n",
    "_KOR_TP = r'(?:ëª©\\s*í‘œ\\s*ì£¼\\s*ê°€|ëª©\\s*í‘œ\\s*ê°€)'\n",
    "_TP_WORD = r'(?<![A-Za-z0-9])tp(?![A-Za-z0-9])'\n",
    "TP_MASK_PATTERN = re.compile(\n",
    "    rf'(?P<kw>{_KOR_TP}|{_TP_WORD})(?P<pp>ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì™€|ê³¼|ë¡œ|ìœ¼ë¡œ)?',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def _particle_fix(pp: str) -> str:\n",
    "    if not pp:\n",
    "        return ''\n",
    "    mapping = {'ëŠ”': 'ì€', 'ê°€': 'ì´', 'ë¥¼': 'ì„', 'ì™€': 'ê³¼', 'ë¡œ': 'ìœ¼ë¡œ'}\n",
    "    return mapping.get(pp, pp)\n",
    "\n",
    "def mask_target_keywords_to_num(text: str) -> str:\n",
    "    def _repl(m: re.Match) -> str:\n",
    "        pp = m.group('pp') or ''\n",
    "        return 'num' + _particle_fix(pp)\n",
    "    return TP_MASK_PATTERN.sub(_repl, text)\n",
    "\n",
    "# ===== ìˆ«ì â†’ 'num' ì¹˜í™˜ =====\n",
    "NUM_PATTERN_TO_NUM = re.compile(\n",
    "    r\"\"\"(?x)\n",
    "    (?:[+-]?\\d{1,3}(?:,\\d{3})+(?:\\.\\d+)?%?)|\n",
    "    (?:[+-]?\\d+\\.\\d+%?)|\n",
    "    (?:[+-]?\\d+%?)|\n",
    "    (?:\\d+\\s*ë§Œì›|\\d+\\s*ì–µì›|\\d+\\s*ì¡°ì›|\\d+\\s*ì›)\n",
    "    \"\"\"\n",
    ")\n",
    "def mask_numbers_to_num(text: str) -> str:\n",
    "    return NUM_PATTERN_TO_NUM.sub(\"num\", text)\n",
    "\n",
    "# ===== ìµœì‹  ë‚ ì§œìˆœ Nê°œ ì„ íƒ =====\n",
    "def read_latest_txts(dirpath: Path, limit: int):\n",
    "    files = [p for p in dirpath.glob(\"*.txt\") if p.is_file()]\n",
    "    def extract_date(fname: str):\n",
    "        nums = re.findall(r\"\\d{8}\", fname)\n",
    "        return int(nums[-1]) if nums else 0\n",
    "    files.sort(key=lambda x: extract_date(x.name), reverse=True)\n",
    "    files = files[:limit]\n",
    "    print(f\"[ì„ íƒ] {dirpath.name}: {len(files)}ê°œ (ìµœì‹ ìˆœ)\")\n",
    "    texts, names = [], []\n",
    "    for p in files:\n",
    "        raw = read_text_smart(p)\n",
    "        if raw is None:\n",
    "            raw = \"\"\n",
    "        texts.append(raw)\n",
    "        names.append(p.name)\n",
    "    return texts, names\n",
    "\n",
    "# ===== ì „ì²˜ë¦¬(ì„¸ì´í”„ê°€ë“œ í¬í•¨) =====\n",
    "def preprocess_full(doc: str) -> str:\n",
    "    merged = merge_all_to_one_line(doc)\n",
    "    corrected = correct_text_single_line(merged) if merged else \"\"\n",
    "    if not corrected:\n",
    "        corrected = merged\n",
    "    step_tp = mask_target_keywords_to_num(corrected)\n",
    "    final_text = mask_numbers_to_num(step_tp).strip()\n",
    "    return final_text\n",
    "\n",
    "def preprocess_with_safeguard(doc: str) -> str:\n",
    "    # A) FULL\n",
    "    t = preprocess_full(doc)\n",
    "    if len(t) >= MIN_CHARS_AFTER:\n",
    "        return t\n",
    "    # B) êµì •ë§Œ\n",
    "    merged = merge_all_to_one_line(doc)\n",
    "    only_correct = correct_text_single_line(merged) if merged else \"\"\n",
    "    if len(only_correct) >= MIN_CHARS_AFTER:\n",
    "        return only_correct\n",
    "    # C) ë³‘í•©ë§Œ\n",
    "    if len(merged) >= MIN_CHARS_AFTER:\n",
    "        return merged\n",
    "    # D) ê¸°ë³¸ì •ë¦¬ë§Œ(ê°œí–‰ ì œê±° X)\n",
    "    basic = basic_trim_keep_newlines(doc)\n",
    "    if len(basic) >= MIN_CHARS_AFTER:\n",
    "        return basic\n",
    "    # E) ê·¸ë˜ë„ ì§§ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´\n",
    "    return \"\"\n",
    "\n",
    "def main():\n",
    "    # 0) ë°ì´í„° ë¡œë“œ(ìµœì‹  Nê°œì”©)\n",
    "    neg_raw, neg_names = read_latest_txts(NEG_DIR, limit=LIMIT_PER_CLASS)\n",
    "    pos_raw, pos_names = read_latest_txts(POS_DIR, limit=LIMIT_PER_CLASS)\n",
    "\n",
    "    X_raw = neg_raw + pos_raw\n",
    "    y     = [0]*len(neg_raw) + [1]*len(pos_raw)\n",
    "    names = neg_names + pos_names\n",
    "\n",
    "    # 1) ì „ì²˜ë¦¬ + ì„¸ì´í”„ê°€ë“œ\n",
    "    X_clean, y_clean, kept_names, strat = [], [], [], []\n",
    "    skipped = 0\n",
    "\n",
    "    for doc, label, fname in zip(X_raw, y, names):\n",
    "        t = preprocess_full(doc)\n",
    "        s = \"FULL\"\n",
    "        if len(t) < MIN_CHARS_AFTER:\n",
    "            t = correct_text_single_line(merge_all_to_one_line(doc))\n",
    "            s = \"CORRECT_ONLY\"\n",
    "        if not t or len(t) < MIN_CHARS_AFTER:\n",
    "            t = merge_all_to_one_line(doc)\n",
    "            s = \"MERGE_ONLY\"\n",
    "        if not t or len(t) < MIN_CHARS_AFTER:\n",
    "            t = basic_trim_keep_newlines(doc)\n",
    "            s = \"BASIC_ONLY\"\n",
    "\n",
    "        if not t or len(t) < MIN_CHARS_AFTER:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        X_clean.append(t)\n",
    "        y_clean.append(label)\n",
    "        kept_names.append(fname)\n",
    "        strat.append(s)\n",
    "\n",
    "    print(f\"[ì„¸ì´í”„ê°€ë“œ] ìµœì¢… ìœ ì§€ {len(X_clean)}ê°œ / ìŠ¤í‚µ {skipped}ê°œ (ê¸°ì¤€: {MIN_CHARS_AFTER}ì)\")\n",
    "\n",
    "    # 2) ì €ì¥\n",
    "    (OUT_DIR / \"preprocessed.json\").write_text(\n",
    "        json.dumps({\"texts\": X_clean, \"labels\": y_clean}, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    with open(OUT_DIR / \"preprocess_debug.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"filename\\tstrategy\\tlen\\n\")\n",
    "        for txt, s, nm in zip(X_clean, strat, kept_names):\n",
    "            f.write(f\"{nm}\\t{s}\\t{len(txt)}\\n\")\n",
    "\n",
    "    print(f\"[ì €ì¥ ì™„ë£Œ] {OUT_DIR / 'preprocessed.json'}\")\n",
    "    print(f\"[ë””ë²„ê·¸] {OUT_DIR / 'preprocess_debug.tsv'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30c303e-eba1-43da-97dc-46ef3287192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bsbcn\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[ë°ì´í„°] ì´ 4671ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n",
      "[ë¶„í• ] train=4203, val=468 (stratified)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cd91d78d944f34b990e82dc16f56f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079e225946f544f085776baa275c3c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-FinBERT-SC and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\bsbcn\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\bsbcn\\AppData\\Local\\Temp\\ipykernel_18520\\1200860915.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 06:40, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.550300</td>\n",
       "      <td>0.385275</td>\n",
       "      <td>0.839744</td>\n",
       "      <td>0.839532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.363955</td>\n",
       "      <td>0.850427</td>\n",
       "      <td>0.850096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.401113</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.845022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>0.414853</td>\n",
       "      <td>0.837607</td>\n",
       "      <td>0.837417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.427412</td>\n",
       "      <td>0.835470</td>\n",
       "      <td>0.835301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì™„ë£Œ] ëª¨ë¸ ì €ì¥: C:\\Users\\bsbcn\\Desktop\\ê²°ê³¼_ë§¤ìˆ˜ë§¤ë„_í•™ìŠµ\\final_model\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===== ê²½ë¡œ =====\n",
    "HOME = Path.home()\n",
    "OUT_DIR = HOME / \"Desktop\" / \"ê²°ê³¼_ë§¤ìˆ˜ë§¤ë„_í•™ìŠµ\"\n",
    "DATA_PATH = OUT_DIR / \"preprocessed.json\"\n",
    "SEED = 42\n",
    "\n",
    "# ===== ë°ì´í„° ë¡œë“œ =====\n",
    "with open(DATA_PATH, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "X, y = data[\"texts\"], data[\"labels\"]\n",
    "print(f\"[ë°ì´í„°] ì´ {len(X)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ===== stratified split (ì…”í”Œ + ê³„ì¸µ) =====\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=SEED, stratify=y, shuffle=True\n",
    ")\n",
    "print(f\"[ë¶„í• ] train={len(X_train)}, val={len(X_val)} (stratified)\")\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "val_ds   = Dataset.from_dict({\"text\": X_val,   \"label\": y_val})\n",
    "\n",
    "# ===== í† í¬ë‚˜ì´ì € / ëª¨ë¸ =====\n",
    "MODEL_NAME = \"snunlp/KR-FinBERT-SC\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "train_tok = train_ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "val_tok   = val_ds.map(tok_fn,   batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# ===== í‰ê°€ ì§€í‘œ =====\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR / \"checkpoints\"),\n",
    "    learning_rate=2e-5,                  \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,      \n",
    "    num_train_epochs=6,                   \n",
    "    warmup_ratio=0.06,                   \n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=1e-4)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "save_dir = OUT_DIR / \"final_model\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model(str(save_dir))\n",
    "tokenizer.save_pretrained(str(save_dir))\n",
    "print(f\"[ì™„ë£Œ] ëª¨ë¸ ì €ì¥: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a825b8-d902-4aed-8642-732d002d59b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ë°ì´í„°] ì´ 4671ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n",
      "[ë¶„í• ] train=4203, val=468 (stratified)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3047e8c0aa48d3b38d92eb698c068a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940e2965b09e4a068158e0f0a5a67c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-FinBERT-SC and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\bsbcn\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\bsbcn\\AppData\\Local\\Temp\\ipykernel_23924\\2923973766.py:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LoggingTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[pre-train @epoch0] í•™ìŠµ ì‹œì‘ ì „ ê²€ì¦ í‰ê°€ ì‹¤í–‰...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_f1_macro so early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre-train @epoch0] {'pretrain_loss': 0.7454280853271484, 'pretrain_model_preparation_time': 0.001, 'pretrain_accuracy': 0.4807692307692308, 'pretrain_f1_macro': 0.3941021337808679, 'pretrain_runtime': 2.2936, 'pretrain_samples_per_second': 204.05, 'pretrain_steps_per_second': 6.54}\n",
      "[epoch0 step=0] loss=0.7153 accuracy=0.5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 87/390 01:26 < 05:09, 0.98 it/s, Epoch 1.31/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.391500</td>\n",
       "      <td>0.396625</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.831197</td>\n",
       "      <td>0.830714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch0 step=1] loss=0.7097 accuracy=0.5156\n",
      "[epoch0 step=2] loss=0.7345 accuracy=0.4531\n",
      "[epoch0 step=3] loss=0.7147 accuracy=0.5000\n",
      "[epoch0 step=4] loss=0.7583 accuracy=0.3594\n",
      "[epoch0 step=5] loss=0.7348 accuracy=0.4219\n",
      "[epoch0 step=6] loss=0.7096 accuracy=0.4688\n",
      "[epoch0 step=7] loss=0.7073 accuracy=0.4375\n",
      "[epoch0 step=8] loss=0.6648 accuracy=0.6094\n",
      "[epoch0 step=9] loss=0.6893 accuracy=0.5312\n",
      "[epoch0 step=10] loss=0.6633 accuracy=0.6406\n",
      "[epoch0 step=11] loss=0.6776 accuracy=0.5625\n",
      "[epoch0 step=12] loss=0.6634 accuracy=0.5938\n",
      "[epoch0 step=13] loss=0.6507 accuracy=0.6875\n",
      "[epoch0 step=14] loss=0.5845 accuracy=0.7500\n",
      "[epoch0 step=15] loss=0.6163 accuracy=0.6719\n",
      "[epoch0 step=16] loss=0.6029 accuracy=0.7188\n",
      "[epoch0 step=17] loss=0.5755 accuracy=0.6719\n",
      "[epoch0 step=18] loss=0.6837 accuracy=0.5625\n",
      "[epoch0 step=19] loss=0.5400 accuracy=0.7031\n",
      "[epoch0 step=20] loss=0.5921 accuracy=0.6719\n",
      "[epoch0 step=21] loss=0.5472 accuracy=0.7500\n",
      "[epoch0 step=22] loss=0.5335 accuracy=0.7344\n",
      "[epoch0 step=23] loss=0.5892 accuracy=0.6875\n",
      "[epoch0 step=24] loss=0.6487 accuracy=0.6250\n",
      "[epoch0 step=25] loss=0.5938 accuracy=0.7344\n",
      "[epoch0 step=26] loss=0.6193 accuracy=0.7344\n",
      "[epoch0 step=27] loss=0.5982 accuracy=0.7656\n",
      "[epoch0 step=28] loss=0.4924 accuracy=0.7812\n",
      "[epoch0 step=29] loss=0.5427 accuracy=0.7969\n",
      "[epoch0 step=30] loss=0.6792 accuracy=0.6406\n",
      "[epoch0 step=31] loss=0.4611 accuracy=0.7812\n",
      "[epoch0 step=32] loss=0.5740 accuracy=0.7188\n",
      "[epoch0 step=33] loss=0.4936 accuracy=0.7969\n",
      "[epoch0 step=34] loss=0.3524 accuracy=0.8750\n",
      "[epoch0 step=35] loss=0.6056 accuracy=0.6875\n",
      "[epoch0 step=36] loss=0.5304 accuracy=0.7344\n",
      "[epoch0 step=37] loss=0.4416 accuracy=0.8125\n",
      "[epoch0 step=38] loss=0.4588 accuracy=0.8125\n",
      "[epoch0 step=39] loss=0.6391 accuracy=0.6406\n",
      "[epoch0 step=40] loss=0.3734 accuracy=0.8281\n",
      "[epoch0 step=41] loss=0.4727 accuracy=0.7812\n",
      "[epoch0 step=42] loss=0.4480 accuracy=0.8125\n",
      "[epoch0 step=43] loss=0.5057 accuracy=0.7500\n",
      "[epoch0 step=44] loss=0.4518 accuracy=0.7812\n",
      "[epoch0 step=45] loss=0.4523 accuracy=0.8125\n",
      "[epoch0 step=46] loss=0.4646 accuracy=0.7969\n",
      "[epoch0 step=47] loss=0.3987 accuracy=0.8438\n",
      "[epoch0 step=48] loss=0.4054 accuracy=0.8125\n",
      "[epoch0 step=49] loss=0.4245 accuracy=0.7812\n",
      "[epoch0 step=50] loss=0.3756 accuracy=0.8281\n",
      "[epoch0 step=51] loss=0.4224 accuracy=0.8281\n",
      "[epoch0 step=52] loss=0.5059 accuracy=0.7656\n",
      "[epoch0 step=53] loss=0.3174 accuracy=0.8750\n",
      "[epoch0 step=54] loss=0.3134 accuracy=0.9062\n",
      "[epoch0 step=55] loss=0.3781 accuracy=0.8438\n",
      "[epoch0 step=56] loss=0.4559 accuracy=0.8125\n",
      "[epoch0 step=57] loss=0.4158 accuracy=0.8125\n",
      "[epoch0 step=58] loss=0.3429 accuracy=0.8594\n",
      "[epoch0 step=59] loss=0.4164 accuracy=0.8594\n",
      "[epoch0 step=60] loss=0.5239 accuracy=0.7500\n",
      "[epoch0 step=61] loss=0.4107 accuracy=0.8594\n",
      "[epoch0 step=62] loss=0.4586 accuracy=0.8438\n",
      "[epoch0 step=63] loss=0.3930 accuracy=0.8438\n",
      "[epoch0 step=64] loss=0.3915 accuracy=0.8438\n",
      "[epoch0 step=65] loss=0.5208 accuracy=0.7467\n",
      "[epoch0 step=65] loss=0.4566 accuracy=0.8047\n",
      "[epoch0 step=65] loss=0.3702 accuracy=0.8516\n",
      "[epoch0 step=65] loss=0.3729 accuracy=0.8359\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 216\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[pre-train @epoch0] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# ===== í•™ìŠµ =====\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# ===== ì €ì¥ =====\u001b[39;00m\n\u001b[0;32m    219\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m OUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2124\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2125\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2126\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2127\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2128\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2486\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "KR-FinBERT (snunlp/KR-FinBERT-SC) ë¯¸ì„¸ì¡°ì • + ì—í­0 ë°°ì¹˜ ë¡œê¹…\n",
    "A) ì½˜ì†” ì¶œë ¥ (epoch 0 ë°°ì¹˜ë³„)\n",
    "B) CSV ì €ì¥  (epoch 0 ë°°ì¹˜ë³„): ~/Desktop/ê²°ê³¼_ë§¤ìˆ˜ë§¤ë„_í•™ìŠµ/step_logs_epoch0.csv\n",
    "\"\"\"\n",
    "import json, csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===== ê²½ë¡œ/ì„¤ì • =====\n",
    "HOME = Path.home()\n",
    "OUT_DIR = HOME / \"Desktop\" / \"ê²°ê³¼_ë§¤ìˆ˜ë§¤ë„_í•™ìŠµ\"\n",
    "DATA_PATH = OUT_DIR / \"preprocessed.json\"\n",
    "SEED = 42\n",
    "\n",
    "CSV_PATH = OUT_DIR / \"step_logs_epoch0.csv\"  # (B)\n",
    "\n",
    "# ===== ë°ì´í„° ë¡œë“œ =====\n",
    "with open(DATA_PATH, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "X, y = data[\"texts\"], data[\"labels\"]\n",
    "print(f\"[ë°ì´í„°] ì´ {len(X)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ===== stratified split =====\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=SEED, stratify=y, shuffle=True\n",
    ")\n",
    "print(f\"[ë¶„í• ] train={len(X_train)}, val={len(X_val)} (stratified)\")\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "val_ds   = Dataset.from_dict({\"text\": X_val,   \"label\": y_val})\n",
    "\n",
    "# ===== í† í¬ë‚˜ì´ì € / ëª¨ë¸ =====\n",
    "MODEL_NAME = \"snunlp/KR-FinBERT-SC\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "train_tok = train_ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "val_tok   = val_ds.map(tok_fn,   batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# ===== í‰ê°€ ì§€í‘œ =====\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "# ===== CSV ì¤€ë¹„ (B) =====\n",
    "def init_csv(path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"global_step\", \"epoch\", \"loss_step\", \"acc_step\", \"lr\"])\n",
    "\n",
    "init_csv(CSV_PATH)\n",
    "\n",
    "# ===== ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ: compute_lossì—ì„œ ì½˜ì†”+CSV ë¡œê¹… (epoch 0ë§Œ) =====\n",
    "class LoggingTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._buf_correct = 0\n",
    "        self._buf_loss_sum = 0.0\n",
    "        self._buf_count = 0\n",
    "        self._buf_nsamples = 0\n",
    "\n",
    "    def _flush_if_needed(self):\n",
    "        # ë²„í¼ê°€ grad_accum_stepsë§Œí¼ ìŒ“ì˜€ìœ¼ë©´ ìŠ¤í… ìš”ì•½ì„ 1ë²ˆë§Œ ì¶œë ¥\n",
    "        if self._buf_count == self.args.gradient_accumulation_steps:\n",
    "            acc_step  = self._buf_correct / self._buf_nsamples if self._buf_nsamples else float(\"nan\")\n",
    "            loss_step = self._buf_loss_sum / self._buf_nsamples if self._buf_nsamples else float(\"nan\")\n",
    "            lr = (self.optimizer.param_groups[0][\"lr\"]\n",
    "                  if getattr(self, \"optimizer\", None) else -1.0)\n",
    "            # ì£¼ì˜: global_stepì€ optimizer.step() í›„ ì¦ê°€í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” \"í˜„ì¬ step ì§„í–‰ ì¤‘\" ì˜ë¯¸\n",
    "            print(f\"[epoch0 step={self.state.global_step}] loss={loss_step:.4f} accuracy={acc_step:.4f}\")\n",
    "            # CSVë„ í•œ ì¤„ë§Œ ê¸°ë¡í•˜ê³  ì‹¶ë‹¤ë©´ ì—¬ê¸°ì— write ì¶”ê°€\n",
    "            # self._write_csv([self.state.global_step, float(self.state.epoch), float(loss_step), float(acc_step), float(lr)])\n",
    "\n",
    "            # ë²„í¼ ì´ˆê¸°í™”\n",
    "            self._buf_correct = 0\n",
    "            self._buf_loss_sum = 0.0\n",
    "            self._buf_count = 0\n",
    "            self._buf_nsamples = 0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # epoch 0ì—ì„œë§Œ ìˆ˜ì§‘/ì¶œë ¥\n",
    "        cur_epoch = self.state.epoch\n",
    "        if cur_epoch is not None and int(cur_epoch) == 0 and \"labels\" in inputs and hasattr(outputs, \"logits\"):\n",
    "            with torch.no_grad():\n",
    "                logits = outputs.logits\n",
    "                labels = inputs[\"labels\"]\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                n = labels.size(0)\n",
    "                correct = (preds == labels).sum().item()\n",
    "\n",
    "                # ì†ì‹¤ì´ 'mean'ì¼ ë•Œ ìƒ˜í”Œìˆ˜ ê°€ì¤‘ í•©ìœ¼ë¡œ ëˆ„ì \n",
    "                self._buf_correct += correct\n",
    "                self._buf_loss_sum += float(loss.detach().cpu()) * n\n",
    "                self._buf_nsamples += n\n",
    "                self._buf_count += 1\n",
    "\n",
    "                self._flush_if_needed()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "'''\n",
    "\n",
    "class LoggingTrainer(Trainer):\n",
    "    def _write_csv(self, row):\n",
    "        with open(CSV_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            wr = csv.writer(f)\n",
    "            wr.writerow(row)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        try:\n",
    "            # ë°°ì¹˜ ì§€í‘œ ê³„ì‚°\n",
    "            if \"labels\" in inputs and hasattr(outputs, \"logits\"):\n",
    "                with torch.no_grad():\n",
    "                    preds = outputs.logits.argmax(dim=-1)\n",
    "                    acc = (preds == inputs[\"labels\"]).float().mean().item()\n",
    "\n",
    "                # í˜„ì¬ epoch/step\n",
    "                cur_epoch = self.state.epoch  # float ë˜ëŠ” None\n",
    "                gstep = self.state.global_step\n",
    "\n",
    "                # LR (ê°€ë“œ)\n",
    "                lr = None\n",
    "                if getattr(self, \"optimizer\", None):\n",
    "                    lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "                else:\n",
    "                    try:\n",
    "                        lr = self._get_learning_rate()\n",
    "                    except Exception:\n",
    "                        lr = -1.0\n",
    "\n",
    "                # === í•µì‹¬: ì—í­ 0 ë‚´ë¶€ì—ì„œë§Œ ìƒì„¸ ë¡œê¹… ===\n",
    "                if cur_epoch is not None and int(cur_epoch) == 0:\n",
    "                    # A) ì½˜ì†” ì¶œë ¥\n",
    "                    print(f\"[epoch0 step={gstep}] loss={float(loss):.4f} \"\n",
    "                          f\"acc={acc:.4f} lr={float(lr):.6g}\")\n",
    "\n",
    "                    # B) CSV ì €ì¥\n",
    "                    self._write_csv([gstep, float(cur_epoch), float(loss.detach().cpu()), float(acc), float(lr)])\n",
    "\n",
    "                    # (ì„ íƒ) Trainer ë‚´ë¶€ ë¡œê·¸ì—ë„ ë‚¨ê¸°ê³  ì‹¶ìœ¼ë©´ ì£¼ì„ í•´ì œ\n",
    "                    # self.log({\"train/loss_step\": float(loss.detach().cpu()),\n",
    "                    #           \"train/acc_step\": float(acc), \"lr\": float(lr)})\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        '''\n",
    "\n",
    "# ===== í•™ìŠµ ì„¤ì • =====\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR / \"checkpoints\"),\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,        # eff. batch = 64\n",
    "    num_train_epochs=6,                   # 4~6 ì¶”ì²œ\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=1,                      # epoch 0 ë°°ì¹˜ë³„ ì½˜ì†” ë¡œê·¸ ìœ„í•´ 1\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = LoggingTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=1e-4)]\n",
    ")\n",
    "\n",
    "# ===== ì—í­ 0 'ì‹œì‘ ì „' ì‚¬ì „ ê²€ì¦ =====\n",
    "print(\"\\n[pre-train @epoch0] í•™ìŠµ ì‹œì‘ ì „ ê²€ì¦ í‰ê°€ ì‹¤í–‰...\")\n",
    "pre = trainer.evaluate(metric_key_prefix=\"pretrain\")\n",
    "print(f\"[pre-train @epoch0] {pre}\")\n",
    "\n",
    "# ===== í•™ìŠµ =====\n",
    "trainer.train()\n",
    "\n",
    "# ===== ì €ì¥ =====\n",
    "save_dir = OUT_DIR / \"final_model\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model(str(save_dir))\n",
    "tokenizer.save_pretrained(str(save_dir))\n",
    "print(f\"[ì™„ë£Œ] ëª¨ë¸ ì €ì¥: {save_dir}\")\n",
    "print(f\"[CSV ì €ì¥ ì™„ë£Œ] {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4e249-4031-4e4a-96ad-c81aa7c09895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
