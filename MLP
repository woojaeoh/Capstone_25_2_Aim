import json, torch, numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

# ===== 설정 =====
HOME = Path.home()
DATA_PATH = HOME / "Desktop" / "결과_매수매도_학습" / "preprocessed.json"
MODEL_NAME = "snunlp/KR-FinBERT-SC"
BATCH_SIZE = 32
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== 1. 데이터 로드 =====
with open(DATA_PATH, encoding="utf-8") as f:
    data = json.load(f)
X_text, y = data["texts"], data["labels"]

# Stratified Split
X_train_txt, X_val_txt, y_train, y_val = train_test_split(
    X_text, y, test_size=0.1, random_state=42, stratify=y, shuffle=True
)

# ===== 2. FinBERT 인코더 고정 및 임베딩 추출 함수 =====
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
bert_model = AutoModel.from_pretrained(MODEL_NAME).to(device)

# ★ 핵심: 모델 고정 (Freeze)
for param in bert_model.parameters():
    param.requires_grad = False
bert_model.eval()

def get_embeddings(texts, batch_size=32):
    all_embeddings = []
    # 메모리 절약을 위해 배치 단위 처리
    for i in tqdm(range(0, len(texts), batch_size), desc="임베딩 추출 중"):
        batch_texts = texts[i : i + batch_size]
        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = bert_model(**inputs)
            # CLS 토큰(문장 전체의 의미) 벡터 추출 (Batch, 768)
            cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            all_embeddings.append(cls_emb)
            
    return np.vstack(all_embeddings)

print(">>> 훈련 데이터 임베딩 추출 시작...")
X_train_emb = get_embeddings(X_train_txt, BATCH_SIZE)
print(">>> 검증 데이터 임베딩 추출 시작...")
X_val_emb = get_embeddings(X_val_txt, BATCH_SIZE)

print(f"추출 완료: Train shape {X_train_emb.shape}, Val shape {X_val_emb.shape}")
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

print("\n===== [2] MLP (Deep Neural Net) 학습 시작 =====")

# 데이터셋 변환 (Numpy -> Tensor)
train_ds = TensorDataset(torch.tensor(X_train_emb, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))
val_ds = TensorDataset(torch.tensor(X_val_emb, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=32)

# MLP 모델 정의
class SimpleMLP(nn.Module):
    def __init__(self, input_dim=768, num_classes=2):
        super(SimpleMLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(256, num_classes)
        )
        
    def forward(self, x):
        return self.layers(x)

model_mlp = SimpleMLP().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model_mlp.parameters(), lr=1e-3)

# 학습 루프
epochs = 10
best_acc = 0

for epoch in range(epochs):
    model_mlp.train()
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        outputs = model_mlp(xb)
        loss = criterion(outputs, yb)
        loss.backward()
        optimizer.step()
        
    # 검증
    model_mlp.eval()
    val_preds, val_targets = [], []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.to(device)
            outputs = model_mlp(xb)
            preds = torch.argmax(outputs, dim=1)
            val_preds.extend(preds.cpu().numpy())
            val_targets.extend(yb.numpy())
            
    acc = accuracy_score(val_targets, val_preds)
    print(f"Epoch {epoch+1}/{epochs} - Val Acc: {acc:.4f}")
    
    if acc > best_acc:
        best_acc = acc
        torch.save(model_mlp.state_dict(), str(HOME / "Desktop" / "결과_매수매도_학습" / "best_mlp.pt"))

print(f"MLP Best Accuracy: {best_acc:.4f}")
