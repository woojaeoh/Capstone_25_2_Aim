import json, numpy as np
from pathlib import Path
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split

# ===== 경로 =====
HOME = Path.home()
OUT_DIR = HOME / "Desktop" / "결과_매수매도_학습"
DATA_PATH = OUT_DIR / "preprocessed.json"
SEED = 42

# ===== 데이터 로드 =====
with open(DATA_PATH, encoding="utf-8") as f:
    data = json.load(f)
X, y = data["texts"], data["labels"]
print(f"[데이터] 총 {len(X)}개 문서 로드 완료")

# ===== stratified split (셔플 + 계층) =====
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.1, random_state=SEED, stratify=y, shuffle=True
)
print(f"[분할] train={len(X_train)}, val={len(X_val)} (stratified)")

train_ds = Dataset.from_dict({"text": X_train, "label": y_train})
val_ds   = Dataset.from_dict({"text": X_val,   "label": y_val})

# ===== 토크나이저 / 모델 =====
MODEL_NAME = "snunlp/KR-FinBERT-SC"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

def tok_fn(batch):
    return tokenizer(batch["text"], truncation=True, max_length=512)

train_tok = train_ds.map(tok_fn, batched=True, remove_columns=["text"])
val_tok   = val_ds.map(tok_fn,   batched=True, remove_columns=["text"])
data_collator = DataCollatorWithPadding(tokenizer)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=2, ignore_mismatched_sizes=True
)

# ===== 평가 지표 =====
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1_macro": f1_score(labels, preds, average="macro"),
    }

args = TrainingArguments(
    output_dir=str(OUT_DIR / "checkpoints"),
    learning_rate=2e-5,                  
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=4,      
    num_train_epochs=6,                   
    warmup_ratio=0.06,                   
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_steps=50,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    greater_is_better=True,
    fp16=True,
    report_to="none",
    seed=SEED,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_tok,
    eval_dataset=val_tok,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=1e-4)]
)

trainer.train()

save_dir = OUT_DIR / "final_model"
save_dir.mkdir(parents=True, exist_ok=True)
trainer.save_model(str(save_dir))
tokenizer.save_pretrained(str(save_dir))
print(f"[완료] 모델 저장: {save_dir}")
