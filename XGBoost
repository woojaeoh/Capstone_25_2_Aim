import json, torch, numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

# ===== 설정 =====
HOME = Path.home()
DATA_PATH = HOME / "Desktop" / "결과_매수매도_학습" / "preprocessed.json"
MODEL_NAME = "snunlp/KR-FinBERT-SC"
BATCH_SIZE = 32
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== 1. 데이터 로드 =====
with open(DATA_PATH, encoding="utf-8") as f:
    data = json.load(f)
X_text, y = data["texts"], data["labels"]

# Stratified Split
X_train_txt, X_val_txt, y_train, y_val = train_test_split(
    X_text, y, test_size=0.1, random_state=42, stratify=y, shuffle=True
)

# ===== 2. FinBERT 인코더 고정 및 임베딩 추출 함수 =====
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
bert_model = AutoModel.from_pretrained(MODEL_NAME).to(device)

# ★ 핵심: 모델 고정 (Freeze)
for param in bert_model.parameters():
    param.requires_grad = False
bert_model.eval()

def get_embeddings(texts, batch_size=32):
    all_embeddings = []
    # 메모리 절약을 위해 배치 단위 처리
    for i in tqdm(range(0, len(texts), batch_size), desc="임베딩 추출 중"):
        batch_texts = texts[i : i + batch_size]
        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = bert_model(**inputs)
            # CLS 토큰(문장 전체의 의미) 벡터 추출 (Batch, 768)
            cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            all_embeddings.append(cls_emb)
            
    return np.vstack(all_embeddings)

print(">>> 훈련 데이터 임베딩 추출 시작...")
X_train_emb = get_embeddings(X_train_txt, BATCH_SIZE)
print(">>> 검증 데이터 임베딩 추출 시작...")
X_val_emb = get_embeddings(X_val_txt, BATCH_SIZE)

print(f"추출 완료: Train shape {X_train_emb.shape}, Val shape {X_val_emb.shape}")

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, f1_score

print("\n===== [1] XGBoost 학습 시작 (tuned) =====")

xgb = XGBClassifier(
    # 구조
    n_estimators=1200,          
    learning_rate=0.03,       
    max_depth=4,            
    min_child_weight=3,         
    gamma=0.5,              

    # 서브샘플링(규제 + 일반화)
    subsample=0.8,              
    colsample_bytree=0.8,       

    # L2 / L1 규제
    reg_lambda=1.0,            
    reg_alpha=0.0,        
    # 기타
    objective="binary:logistic",
    eval_metric="logloss",    
    random_state=42,
    n_jobs=-1,
    tree_method="hist",         
    early_stopping_rounds=50 
)

xgb.fit(
    X_train_emb, y_train,
    eval_set=[(X_val_emb, y_val)],
    verbose=True
)

# 평가
preds = xgb.predict(X_val_emb)
print(f"XGBoost Accuracy: {accuracy_score(y_val, preds):.4f}")
print(f"XGBoost F1 Score: {f1_score(y_val, preds, average='macro'):.4f}")
